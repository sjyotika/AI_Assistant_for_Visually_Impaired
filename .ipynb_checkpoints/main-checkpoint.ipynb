{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab7a5f8-7463-4eab-9eed-4da0d5aa2810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 model...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 8.1MB/s 0.8s.8s<0.0ss.9s\n",
      "Model loaded successfully!\n",
      "Initializing webcam...\n",
      "Webcam initialized: 640x480\n",
      "Press 'q' to quit\n",
      "Shutting down...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AI Assistant for Visually Impaired Users\n",
    "=========================================\n",
    "\n",
    "INSTALLATION INSTRUCTIONS:\n",
    "1. Install required packages:\n",
    "   pip install opencv-python ultralytics numpy pyttsx3\n",
    "\n",
    "2. Run the script:\n",
    "   python main.py\n",
    "\n",
    "3. Press 'q' to quit the application\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Webcam connected to the system\n",
    "- Python 3.7 or higher\n",
    "- Internet connection for first run (to download YOLOv8n model)\n",
    "\n",
    "This script provides real-time object detection and audio guidance\n",
    "for visually impaired users using a webcam feed.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "\n",
    "\n",
    "class VisionAssistant:\n",
    "    \"\"\"Main class for the vision assistance system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the vision assistant with all necessary components\"\"\"\n",
    "        \n",
    "        # Initialize text-to-speech engine\n",
    "        try:\n",
    "            self.tts_engine = pyttsx3.init()\n",
    "            self.tts_engine.setProperty('rate', 180)  # Speed of speech\n",
    "            self.tts_engine.setProperty('volume', 1.0)  # Volume (0.0 to 1.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing TTS engine: {e}\")\n",
    "            self.tts_engine = None\n",
    "        \n",
    "        # Load YOLOv8 nano model (will download on first run)\n",
    "        try:\n",
    "            print(\"Loading YOLOv8 model...\")\n",
    "            self.model = YOLO('yolov8n.pt')  # Nano model for CPU efficiency\n",
    "            print(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading YOLO model: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Define high-priority object classes (COCO dataset classes)\n",
    "        self.priority_classes = {\n",
    "            'person', 'chair', 'couch', 'bed', 'dining table', \n",
    "            'door', 'car', 'bicycle', 'motorcycle', 'bus', 'truck',\n",
    "            'bottle', 'cup', 'laptop', 'cell phone', 'stairs'\n",
    "        }\n",
    "        \n",
    "        # Tracking system to avoid repeated announcements\n",
    "        self.last_announcement = {}  # {position_key: timestamp}\n",
    "        self.announcement_cooldown = 3.0  # Seconds before re-announcing same object\n",
    "        \n",
    "        # TTS lock to prevent overlapping speech\n",
    "        self.tts_lock = threading.Lock()\n",
    "        self.is_speaking = False\n",
    "        \n",
    "        # Frame dimensions (will be set when video starts)\n",
    "        self.frame_width = 0\n",
    "        self.frame_height = 0\n",
    "    \n",
    "    def calculate_position(self, x_center, bbox_width):\n",
    "        \"\"\"\n",
    "        Determine if object is on left, center, or right of frame\n",
    "        \n",
    "        Args:\n",
    "            x_center: X coordinate of bounding box center\n",
    "            bbox_width: Width of bounding box\n",
    "            \n",
    "        Returns:\n",
    "            str: Position description ('left', 'center', 'right')\n",
    "        \"\"\"\n",
    "        # Divide frame into three zones\n",
    "        left_boundary = self.frame_width * 0.33\n",
    "        right_boundary = self.frame_width * 0.67\n",
    "        \n",
    "        if x_center < left_boundary:\n",
    "            return 'left'\n",
    "        elif x_center > right_boundary:\n",
    "            return 'right'\n",
    "        else:\n",
    "            return 'ahead'\n",
    "    \n",
    "    def estimate_proximity(self, bbox_width, bbox_height):\n",
    "        \"\"\"\n",
    "        Estimate how close an object is based on bounding box size\n",
    "        \n",
    "        Args:\n",
    "            bbox_width: Width of bounding box\n",
    "            bbox_height: Height of bounding box\n",
    "            \n",
    "        Returns:\n",
    "            str: Proximity description ('very close', 'close', 'moderate', 'far')\n",
    "        \"\"\"\n",
    "        # Calculate bounding box area as percentage of frame\n",
    "        bbox_area = bbox_width * bbox_height\n",
    "        frame_area = self.frame_width * self.frame_height\n",
    "        area_ratio = bbox_area / frame_area\n",
    "        \n",
    "        if area_ratio > 0.3:\n",
    "            return 'very close'\n",
    "        elif area_ratio > 0.15:\n",
    "            return 'close'\n",
    "        elif area_ratio > 0.05:\n",
    "            return 'moderate'\n",
    "        else:\n",
    "            return 'far'\n",
    "    \n",
    "    def calculate_priority(self, class_name, proximity, position):\n",
    "        \"\"\"\n",
    "        Calculate priority score for detected objects\n",
    "        \n",
    "        Args:\n",
    "            class_name: Name of detected object\n",
    "            proximity: Proximity level\n",
    "            position: Position in frame\n",
    "            \n",
    "        Returns:\n",
    "            int: Priority score (higher = more important)\n",
    "        \"\"\"\n",
    "        priority = 0\n",
    "        \n",
    "        # Base priority for important objects\n",
    "        if class_name in self.priority_classes:\n",
    "            priority += 10\n",
    "        \n",
    "        # Extra priority for people\n",
    "        if class_name == 'person':\n",
    "            priority += 5\n",
    "        \n",
    "        # Proximity scoring\n",
    "        proximity_scores = {\n",
    "            'very close': 15,\n",
    "            'close': 10,\n",
    "            'moderate': 5,\n",
    "            'far': 0\n",
    "        }\n",
    "        priority += proximity_scores.get(proximity, 0)\n",
    "        \n",
    "        # Position scoring (ahead is most important)\n",
    "        position_scores = {\n",
    "            'ahead': 10,\n",
    "            'left': 5,\n",
    "            'right': 5\n",
    "        }\n",
    "        priority += position_scores.get(position, 0)\n",
    "        \n",
    "        return priority\n",
    "    \n",
    "    def should_announce(self, announcement_key):\n",
    "        \"\"\"\n",
    "        Check if enough time has passed to re-announce this object\n",
    "        \n",
    "        Args:\n",
    "            announcement_key: Unique key for this announcement\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if should announce, False otherwise\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if announcement_key not in self.last_announcement:\n",
    "            self.last_announcement[announcement_key] = current_time\n",
    "            return True\n",
    "        \n",
    "        time_since_last = current_time - self.last_announcement[announcement_key]\n",
    "        \n",
    "        if time_since_last >= self.announcement_cooldown:\n",
    "            self.last_announcement[announcement_key] = current_time\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def speak_async(self, text):\n",
    "        \"\"\"\n",
    "        Speak text asynchronously to avoid blocking video processing\n",
    "        \n",
    "        Args:\n",
    "            text: Text to speak\n",
    "        \"\"\"\n",
    "        if not self.tts_engine or self.is_speaking:\n",
    "            return\n",
    "        \n",
    "        def speak_thread():\n",
    "            with self.tts_lock:\n",
    "                self.is_speaking = True\n",
    "                try:\n",
    "                    self.tts_engine.say(text)\n",
    "                    self.tts_engine.runAndWait()\n",
    "                except Exception as e:\n",
    "                    print(f\"TTS error: {e}\")\n",
    "                finally:\n",
    "                    self.is_speaking = False\n",
    "        \n",
    "        # Start speech in separate thread\n",
    "        thread = threading.Thread(target=speak_thread, daemon=True)\n",
    "        thread.start()\n",
    "    \n",
    "    def process_detections(self, results):\n",
    "        \"\"\"\n",
    "        Process YOLO detection results and generate guidance\n",
    "        \n",
    "        Args:\n",
    "            results: YOLO detection results\n",
    "            \n",
    "        Returns:\n",
    "            list: Processed detection data for visualization\n",
    "        \"\"\"\n",
    "        detections = []\n",
    "        \n",
    "        # Extract detection information\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            \n",
    "            for box in boxes:\n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                \n",
    "                # Calculate center and dimensions\n",
    "                x_center = (x1 + x2) / 2\n",
    "                y_center = (y1 + y2) / 2\n",
    "                bbox_width = x2 - x1\n",
    "                bbox_height = y2 - y1\n",
    "                \n",
    "                # Get class name and confidence\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = self.model.names[class_id]\n",
    "                confidence = float(box.conf[0])\n",
    "                \n",
    "                # Only process detections with confidence > 0.5\n",
    "                if confidence < 0.5:\n",
    "                    continue\n",
    "                \n",
    "                # Determine position and proximity\n",
    "                position = self.calculate_position(x_center, bbox_width)\n",
    "                proximity = self.estimate_proximity(bbox_width, bbox_height)\n",
    "                \n",
    "                # Calculate priority\n",
    "                priority = self.calculate_priority(class_name, proximity, position)\n",
    "                \n",
    "                detections.append({\n",
    "                    'bbox': (int(x1), int(y1), int(x2), int(y2)),\n",
    "                    'class_name': class_name,\n",
    "                    'confidence': confidence,\n",
    "                    'position': position,\n",
    "                    'proximity': proximity,\n",
    "                    'priority': priority,\n",
    "                    'center': (int(x_center), int(y_center))\n",
    "                })\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def generate_guidance(self, detections):\n",
    "        \"\"\"\n",
    "        Generate audio guidance based on detected objects\n",
    "        \n",
    "        Args:\n",
    "            detections: List of processed detections\n",
    "        \"\"\"\n",
    "        if not detections:\n",
    "            # Clear path - only announce occasionally\n",
    "            if self.should_announce('clear_path'):\n",
    "                self.speak_async(\"Clear path\")\n",
    "            return\n",
    "        \n",
    "        # Sort by priority (highest first)\n",
    "        detections.sort(key=lambda x: x['priority'], reverse=True)\n",
    "        \n",
    "        # Announce top priority objects (max 2 to avoid overwhelming)\n",
    "        announced_count = 0\n",
    "        max_announcements = 2\n",
    "        \n",
    "        for detection in detections:\n",
    "            if announced_count >= max_announcements:\n",
    "                break\n",
    "            \n",
    "            class_name = detection['class_name']\n",
    "            position = detection['position']\n",
    "            proximity = detection['proximity']\n",
    "            \n",
    "            # Create unique announcement key\n",
    "            announcement_key = f\"{class_name}_{position}_{proximity}\"\n",
    "            \n",
    "            # Check if we should announce this\n",
    "            if not self.should_announce(announcement_key):\n",
    "                continue\n",
    "            \n",
    "            # Generate announcement text\n",
    "            if proximity == 'very close':\n",
    "                message = f\"Warning! {class_name} very close {position}\"\n",
    "            elif proximity == 'close':\n",
    "                message = f\"{class_name} {position}\"\n",
    "            else:\n",
    "                # Only announce moderate/far objects if they're high priority\n",
    "                if detection['priority'] < 20:\n",
    "                    continue\n",
    "                message = f\"{class_name} {position}\"\n",
    "            \n",
    "            # Speak the message\n",
    "            self.speak_async(message)\n",
    "            announced_count += 1\n",
    "    \n",
    "    def draw_detections(self, frame, detections):\n",
    "        \"\"\"\n",
    "        Draw bounding boxes and labels on frame for visualization\n",
    "        \n",
    "        Args:\n",
    "            frame: Video frame\n",
    "            detections: List of processed detections\n",
    "            \n",
    "        Returns:\n",
    "            frame: Annotated frame\n",
    "        \"\"\"\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2 = detection['bbox']\n",
    "            class_name = detection['class_name']\n",
    "            confidence = detection['confidence']\n",
    "            position = detection['position']\n",
    "            proximity = detection['proximity']\n",
    "            \n",
    "            # Color based on proximity\n",
    "            color_map = {\n",
    "                'very close': (0, 0, 255),    # Red\n",
    "                'close': (0, 165, 255),       # Orange\n",
    "                'moderate': (0, 255, 255),    # Yellow\n",
    "                'far': (0, 255, 0)            # Green\n",
    "            }\n",
    "            color = color_map.get(proximity, (255, 255, 255))\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Create label\n",
    "            label = f\"{class_name} ({position})\"\n",
    "            \n",
    "            # Draw label background\n",
    "            (label_width, label_height), _ = cv2.getTextSize(\n",
    "                label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2\n",
    "            )\n",
    "            cv2.rectangle(\n",
    "                frame, \n",
    "                (x1, y1 - label_height - 10), \n",
    "                (x1 + label_width, y1), \n",
    "                color, \n",
    "                -1\n",
    "            )\n",
    "            \n",
    "            # Draw label text\n",
    "            cv2.putText(\n",
    "                frame, \n",
    "                label, \n",
    "                (x1, y1 - 5), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                0.6, \n",
    "                (0, 0, 0), \n",
    "                2\n",
    "            )\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main loop for the vision assistant\"\"\"\n",
    "        \n",
    "        # Initialize webcam\n",
    "        print(\"Initializing webcam...\")\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open webcam\")\n",
    "            print(\"Please ensure your webcam is connected and not in use by another application\")\n",
    "            return\n",
    "        \n",
    "        # Get frame dimensions\n",
    "        self.frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        print(f\"Webcam initialized: {self.frame_width}x{self.frame_height}\")\n",
    "        print(\"Press 'q' to quit\")\n",
    "        \n",
    "        # Initial greeting\n",
    "        self.speak_async(\"Vision assistant started\")\n",
    "        \n",
    "        # Main processing loop\n",
    "        try:\n",
    "            while True:\n",
    "                # Capture frame\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    print(\"Error: Failed to capture frame\")\n",
    "                    break\n",
    "                \n",
    "                # Run YOLOv8 inference\n",
    "                results = self.model(frame, verbose=False, device='cpu')\n",
    "                \n",
    "                # Process detections\n",
    "                detections = self.process_detections(results)\n",
    "                \n",
    "                # Generate audio guidance\n",
    "                self.generate_guidance(detections)\n",
    "                \n",
    "                # Draw detections on frame\n",
    "                annotated_frame = self.draw_detections(frame, detections)\n",
    "                \n",
    "                # Add instruction text\n",
    "                cv2.putText(\n",
    "                    annotated_frame,\n",
    "                    \"Press 'q' to quit\",\n",
    "                    (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    (255, 255, 255),\n",
    "                    2\n",
    "                )\n",
    "                \n",
    "                # Display frame\n",
    "                cv2.imshow('Vision Assistant', annotated_frame)\n",
    "                \n",
    "                # Check for quit key\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nInterrupted by user\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during execution: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            print(\"Shutting down...\")\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            if self.tts_engine:\n",
    "                self.speak_async(\"Vision assistant stopped\")\n",
    "                time.sleep(1)  # Allow final message to complete\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Entry point for the application\"\"\"\n",
    "    try:\n",
    "        assistant = VisionAssistant()\n",
    "        assistant.run()\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error: {e}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Ensure all packages are installed: pip install opencv-python ultralytics numpy pyttsx3\")\n",
    "        print(\"2. Check that your webcam is connected and accessible\")\n",
    "        print(\"3. Ensure you have internet connection for first run (model download)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96135669-073d-47d2-9b2d-719c6114796b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
